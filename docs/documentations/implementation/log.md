# 5.4 Log

# 5.4 日志

A log for a topic named "my_topic" with two partitions consists of two directories (namely ```my_topic_0``` and ```my_topic_1```) populated with data files containing the messages for that topic. The format of the log files is a sequence of "log entries""; each log entry is a 4 byte integer N storing the message length which is followed by the N message bytes. Each message is uniquely identified by a 64-bit integer offset giving the byte position of the start of this message in the stream of all messages ever sent to that topic on that partition. The on-disk format of each message is given below. Each log file is named with the offset of the first message it contains. So the first file created will be 00000000000.kafka, and each additional file will have an integer name roughly S bytes from the previous file where S is the max log file size given in the configuration.

具有两个分区的名为“my_topic”的主题的日志由两个目录（即```my_topic_0```和```my_topic_1```）组成，其中包含用于该主题的消息的数据文件。日志文件的格式是一系列“日志条目（log entry）”；每个日志条目是一个4字节的整数N，存储消息长度，后面跟着N个消息字节。每条消息由一个64位整数偏移量唯一标识，其给出了该消息开始的字节位置，该消息是在该分区上发送给该主题的所有消息流中的，每个消息的磁盘格式如下：每个日志文件用第一个消息的偏移量命名。因此，第一个创建的文件将是00000000000.kafka，每个附加文件将具有从前一个文件而来的大约S字节的整数名称，S是配置中给出的日志文件最大大小。

The exact binary format for records is versioned and maintained as a standard interface so record batches can be transferred between producer, broker, and client without recopying or conversion when desirable. The previous section included details about the on-disk format of records.

record的确切二进制格式是作为标准接口版本化和维护的，因此record batchs可以在生产者，代理和客户端之间传输，而无需在需要时重新复制或转换。前一部分讲解中包含有关record的磁盘格式的详细信息。

The use of the message offset as the message id is unusual. Our original idea was to use a GUID generated by the producer, and maintain a mapping from GUID to offset on each broker. But since a consumer must maintain an ID for each server, the global uniqueness of the GUID provides no value. Furthermore, the complexity of maintaining the mapping from a random id to an offset requires a heavy weight index structure which must be synchronized with disk, essentially requiring a full persistent random-access data structure. Thus to simplify the lookup structure we decided to use a simple per-partition atomic counter which could be coupled with the partition id and node id to uniquely identify a message; this makes the lookup structure simpler, though multiple seeks per consumer request are still likely. However once we settled on a counter, the jump to directly using the offset seemed natural—both after all are monotonically increasing integers unique to a partition. Since the offset is hidden from the consumer API this decision is ultimately an implementation detail and we went with the more efficient approach.

将消息偏移量用作消息ID是不常见的。我们最初的想法是使用由生产者生成的GUID，并在每个代理上维护从GUID到偏移量的映射。但是由于消费者必须为每个服务器维护一个ID，所以GUID的全局唯一性没有提供任何价值。此外，维护从随机ID到偏移量的映射的复杂性需要大量的索引结构，其必须与磁盘同步，本质上需要完全持久的随机访问数据结构。因此，为了简化查找​​结构，我们决定使用一个简单的每个分区原子计数器，它可以与分区ID和节点ID相结合来唯一标识一条消息；这使得查找结构更简单，尽管每个消费者请求的多个搜索仍然是可能的。然而，一旦我们确定了一个计数器，直接使用偏移的跳转看起来很自然 - 毕竟这是一个单独增加的独立于分区的整数。由于消费者API隐藏了偏移量，所以这个决定最终是一个实现细节，我们采用了更高效的方法。

![](../../imgs/kafka_log.png)

## Writes

The log allows serial appends which always go to the last file. This file is rolled over to a fresh file when it reaches a configurable size (say 1GB). The log takes two configuration parameters: M, which gives the number of messages to write before forcing the OS to flush the file to disk, and S, which gives a number of seconds after which a flush is forced. This gives a durability guarantee of losing at most M messages or S seconds of data in the event of a system crash.

该日志允许序列附加，附加内容总是加到最后一个文件中。当文件达到配置的大小（比如1GB）时，该文件将被转存到新文件中。该日志有两个配置参数：M，它给出了在强制操作系统将文件刷新到磁盘之前要写入的消息数量，以及S，它在强制刷新之后给出了秒数。这提供了在系统崩溃时最多丢失M条消息或S秒数据的持久性保证。

## Reads

Reads are done by giving the 64-bit logical offset of a message and an S-byte max chunk size. This will return an iterator over the messages contained in the S-byte buffer. S is intended to be larger than any single message, but in the event of an abnormally large message, the read can be retried multiple times, each time doubling the buffer size, until the message is read successfully. A maximum message and buffer size can be specified to make the server reject messages larger than some size, and to give a bound to the client on the maximum it needs to ever read to get a complete message. It is likely that the read buffer ends with a partial message, this is easily detected by the size delimiting.

读取是通过给出消息的64位逻辑偏移量和S字节的最大块大小来完成的。这将返回包含在S字节缓冲区中的消息的迭代器。S旨在比任何单个消息都大，但是在发生异常大的消息时，可以多次重试读取，每次将缓冲区大小翻倍，直到消息被成功读取。可以指定最大消息和缓冲区大小，以使服务器拒绝大于某个大小的消息，并为客户端提供最大限度的绑定，以便获取完整的消息。读取缓冲区可能在读取到部分消息就结束了，但这很容易通过大小分界来检测。

The actual process of reading from an offset requires first locating the log segment file in which the data is stored, calculating the file-specific offset from the global offset value, and then reading from that file offset. The search is done as a simple binary search variation against an in-memory range maintained for each file.

从偏移量读取的实际过程要求首先定位存储数据的日志段文件，从全局偏移值计算文件特定的偏移量，然后从该文件偏移量中读取。搜索是按照针对每个文件保存的内存范围的简单二进制搜索变化完成的。

The log provides the capability of getting the most recently written message to allow clients to start subscribing as of "right now". This is also useful in the case the consumer fails to consume its data within its SLA-specified number of days. In this case when the client attempts to consume a non-existent offset it is given an OutOfRangeException and can either reset itself or fail as appropriate to the use case.

该日志提供了获取最近写入的消息的能力，以允许客户从“现在”开始订阅。这对于消费者未能在其SLA-specified的天数内使用其数据的情况也是有用的。在这种情况下，当客户端尝试使用不存在的偏移量时，会给它一个OutOfRangeException，并可以根据用例重置自身或运行失败。

The following is the format of the results sent to the consumer.

以下是发送给消费者的结果格式。

```
MessageSetSend (fetch result)
 
total length     : 4 bytes
error code       : 2 bytes
message 1        : x bytes
...
message n        : x bytes
```

```
MultiMessageSetSend (multiFetch result)
 
total length       : 4 bytes
error code         : 2 bytes
messageSetSend 1
...
messageSetSend n
```

## Deletes

Data is deleted one log segment at a time. The log manager allows pluggable delete policies to choose which files are eligible for deletion. The current policy deletes any log with a modification time of more than N days ago, though a policy which retained the last N GB could also be useful. To avoid locking reads while still allowing deletes that modify the segment list we use a copy-on-write style segment list implementation that provides consistent views to allow a binary search to proceed on an immutable static snapshot view of the log segments while deletes are progressing.

某一时刻一个日志段的数据被删除。日志管理器允许可插入的删除策略来选择哪些文件符合删除条件。当前策略会在修改时间超过N天前删除任何日志，但保留最后N GB的策略也可能有用。为了避免锁定读取，同时仍允许删除修改段列表，我们使用了写时复制（copy-on-write）样式段列表实现，它提供了一致的视图，以允许二进制搜索在日志段的不可变静态快照视图上继续进行，同时删除也能进行。

## Guarantees

The log provides a configuration parameter M which controls the maximum number of messages that are written before forcing a flush to disk. On startup a log recovery process is run that iterates over all messages in the newest log segment and verifies that each message entry is valid. A message entry is valid if the sum of its size and offset are less than the length of the file AND the CRC32 of the message payload matches the CRC stored with the message. In the event corruption is detected the log is truncated to the last valid offset.

该日志提供了一个配置参数M，用于控制强制刷新到磁盘之前写入的最大消息数。在启动时，将运行一个日志恢复进程，该进程将遍历最新日志段中的所有消息并验证每个消息条目是否有效。如果消息条目的大小和偏移量之和小于文件的长度，则消息条目有效，并且消息有效载荷的CRC32与存储在消息中的CRC相匹配。如果检测到损坏，日志将被截断为最后一个有效偏移量。

Note that two kinds of corruption must be handled: truncation in which an unwritten block is lost due to a crash, and corruption in which a nonsense block is ADDED to the file. The reason for this is that in general the OS makes no guarantee of the write order between the file inode and the actual block data so in addition to losing written data the file can gain nonsense data if the inode is updated with a new size but a crash occurs before the block containing that data is written. The CRC detects this corner case, and prevents it from corrupting the log (though the unwritten messages are, of course, lost).

请注意，必须处理两种损坏：由于崩溃导致未写入块丢失的截断以及将无意义块添加到文件的损坏。这是因为操作系统一般不能保证文件索引节点和实际块数据之间的写入顺序，所以除了丢失写入的数据之外，如果索引节点更新为新的大小，文件可以获得无意义的数据，但是在写入包含该数据的块之前发生崩溃。CRC检测到这个角落的情况，并防止它破坏日志（尽管未写入的消息当然会丢失）。